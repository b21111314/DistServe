import torch
import numpy as np
from torch.classes.gpt_ops import Qwen3Op
from scripts.encode_input import encode_text
from scripts.decode_output import decode_tokens
from scripts.gpt_token_encoder import get_encoder

# === é…ç½®æ¨¡å‹è¶…å‚æ•°ï¼ˆåº”ä¸ Qwen3-8B é…ç½®ä¸€è‡´ï¼‰ ===
num_layers = 32
hidden_size = 4096
num_heads = 32
num_kv_heads = 32
vocab_size = 151936
inter_size = 11008
max_batch_size = 1
max_seq_len = 2048
block_size = 16
inference_dtype = "fp16"

# === åŠ è½½ tokenizer ===
encoder = get_encoder()  # ä½¿ç”¨ Qwen3 tokenizer çš„ bpe åˆ†è¯å™¨
prompt = "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
input_ids = encode_text(prompt, encoder)  # List[int]
input_ids = torch.tensor([input_ids], dtype=torch.int32, device="cuda")  # (1, seq_len)

# === åˆå§‹åŒ– Qwen3 æ¨¡å‹ ===
model = Qwen3Op(
    num_layers, hidden_size, num_heads, num_kv_heads,
    vocab_size, inter_size, inference_dtype,
    max_batch_size, max_seq_len, [block_size]
)

model.load_weight("path/to/your/qwen3-8b-swift-weights")  # ğŸ‘ˆ ä¿®æ”¹ä¸ºä½ çš„æƒé‡ç›®å½•

# === æ„é€ ç¼“å­˜ KV cache / block table ===
batch_size, seq_len = input_ids.shape
total_tokens = seq_len
num_blocks = (seq_len + block_size - 1) // block_size

# KV cache åˆå§‹åŒ–
k_cache = torch.zeros((num_layers, max_batch_size, num_blocks, block_size, hidden_size // num_heads), dtype=torch.float16, device="cuda")
v_cache = torch.zeros_like(k_cache)

# Block table åˆå§‹åŒ–ï¼ˆç´¢å¼•æ˜ å°„ï¼‰
block_table = torch.arange(num_blocks, dtype=torch.int32, device="cuda").unsqueeze(0).repeat(batch_size, 1)  # (B, n_blocks)

# First token indexï¼ˆå³ seq_len-1ï¼‰
first_token_indexes = torch.tensor([seq_len - 1], dtype=torch.int32, device="cuda")

# === æ¨¡å‹æ¨ç† ===
output = model.forward(input_ids, first_token_indexes, k_cache, v_cache, block_table)  # (1, 1)
output_ids = output[0].tolist()  # æå–å•ä¸ªè¾“å‡º token id

# === è§£ç è¾“å‡º ===
text = decode_tokens(output_ids, encoder)
print("ğŸŒŸ æ¨¡å‹å›å¤:", text)
